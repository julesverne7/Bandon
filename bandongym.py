# -*- coding: utf-8 -*-
"""Bandongym.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F1q2c1c_LJr8DfXocxbbnJ9LVAj_Tn2O
"""

import pandas as pd
from openai import OpenAI
import json
import time
from datetime import datetime

# Initialize OpenAI client
client = OpenAI(api_key='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')  # Replace with your actual API key

# Define the categories
CATEGORIES = [
    'cleanliness',
    'crowding',
    'customer_service',
    'equipment_quality',
    'membership_billing',
    'price',
    'staff_attitude'
]

def analyze_review(review_text, place_name):
    """
    Send a review to ChatGPT and get structured categorized feedback
    """
    prompt = f"""Analyze this gym review and extract specific mentions for each category.
For each category, identify:
1. Specific fragments/quotes from the review that relate to that category
2. Sentiment (positive/negative/neutral)
3. Intensity (1-5, where 5 is very strong sentiment)

Categories to analyze:
- cleanliness
- crowding
- customer_service
- equipment_quality
- membership_billing
- price
- staff_attitude

Review: "{review_text}"

Return your analysis in this exact JSON format:
{{
  "cleanliness": {{"fragments": ["quote1", "quote2"], "sentiment": "negative", "intensity": 3}},
  "crowding": {{"fragments": [], "sentiment": "neutral", "intensity": 0}},
  ...
}}

If a category is not mentioned, use empty fragments array, "neutral" sentiment, and 0 intensity.
IMPORTANT: Return ONLY the JSON object, no additional text."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # or "gpt-4o" for better quality
            messages=[
                {"role": "system", "content": "You are an expert at analyzing customer reviews and extracting structured feedback. Always return valid JSON only."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )

        # Parse the JSON response
        result = json.loads(response.choices[0].message.content)
        return result

    except Exception as e:
        print(f"Error analyzing review: {e}")
        return None

def process_reviews(input_file, output_file):
    """
    Process all reviews and save results to Excel
    """
    # Read the Excel file
    print(f"Reading {input_file}...")
    df = pd.read_excel(input_file)

    print(f"Found {len(df)} reviews to process")

    # Prepare columns for results
    result_columns = {}
    for category in CATEGORIES:
        result_columns[f'{category}_fragments'] = []
        result_columns[f'{category}_sentiment'] = []
        result_columns[f'{category}_intensity'] = []

    # Add a processing status column
    result_columns['processing_status'] = []

    # Process each review
    for idx, row in df.iterrows():
        print(f"\nProcessing review {idx + 1}/{len(df)} - {row['PLACE NAME']}")

        review_text = row['TEXT']
        place_name = row['PLACE NAME']

        # Analyze the review
        analysis = analyze_review(review_text, place_name)

        if analysis:
            # Extract data for each category
            for category in CATEGORIES:
                cat_data = analysis.get(category, {})
                fragments = cat_data.get('fragments', [])
                sentiment = cat_data.get('sentiment', 'neutral')
                intensity = cat_data.get('intensity', 0)

                # Join fragments with ' | ' separator
                result_columns[f'{category}_fragments'].append(' | '.join(fragments) if fragments else '')
                result_columns[f'{category}_sentiment'].append(sentiment)
                result_columns[f'{category}_intensity'].append(intensity)

            result_columns['processing_status'].append('success')
        else:
            # If analysis failed, fill with empty values
            for category in CATEGORIES:
                result_columns[f'{category}_fragments'].append('')
                result_columns[f'{category}_sentiment'].append('error')
                result_columns[f'{category}_intensity'].append(0)

            result_columns['processing_status'].append('failed')

        # Add a small delay to avoid rate limits
        time.sleep(0.5)

        # Save progress every 10 reviews
        if (idx + 1) % 10 == 0:
            print(f"Saving progress at review {idx + 1}...")
            temp_df = df.copy()
            for col_name, col_data in result_columns.items():
                temp_df[col_name] = col_data + [''] * (len(df) - len(col_data))
            temp_df.to_excel(f'{output_file}', index=False)

    # Add all result columns to the dataframe
    for col_name, col_data in result_columns.items():
        df[col_name] = col_data

    # Save final results
    print(f"\nSaving final results to {output_file}...")
    df.to_excel(output_file, index=False)
    print("Done!")

    return df

# Main execution
if __name__ == "__main__":
    input_file = "/content/sample_data/Bandon Locations_Full Data_1.xlsx"
    output_file = f"Analyzed_Reviews_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

    # Process the reviews
    results_df = process_reviews(input_file, output_file)

    # Print summary statistics
    print("\n=== SUMMARY STATISTICS ===")
    print(f"Total reviews processed: {len(results_df)}")
    print(f"Successful: {sum(results_df['processing_status'] == 'success')}")
    print(f"Failed: {sum(results_df['processing_status'] == 'failed')}")

    # Show sentiment breakdown by category
    print("\n=== SENTIMENT BREAKDOWN BY CATEGORY ===")
    for category in CATEGORIES:
        sentiment_col = f'{category}_sentiment'
        print(f"\n{category.upper()}:")
        print(results_df[sentiment_col].value_counts())

analyzed_file = "temp_Analyzed_Reviews_20251008_151005.xlsx"  # Your actual file

# pip install matplotlib seaborn numpy

# python review_visualizer.py

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime

# Set style for better-looking plots
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

CATEGORIES = [
    'cleanliness',
    'crowding',
    'customer_service',
    'equipment_quality',
    'membership_billing',
    'price',
    'staff_attitude'
]

def load_analyzed_data(file_path):
    """Load the analyzed Excel file"""
    df = pd.read_excel(file_path)
    return df

def calculate_negative_percentage(df):
    """
    Calculate percentage of negative mentions by location and category
    """
    locations = df['PLACE ADDRESS'].unique()

    # Create a matrix for the heatmap
    heatmap_data = []
    location_names = []

    for location in locations:
        location_df = df[df['PLACE ADDRESS'] == location]
        row_data = []

        # Extract city name from address for cleaner labels
        city = location.split(',')[1].strip() if ',' in location else location[:30]
        location_names.append(city)

        for category in CATEGORIES:
            sentiment_col = f'{category}_sentiment'

            # Count mentions of this category
            total_mentions = (location_df[sentiment_col] != 'neutral').sum()
            negative_mentions = (location_df[sentiment_col] == 'negative').sum()

            # Calculate percentage (avoid division by zero)
            if total_mentions > 0:
                negative_pct = (negative_mentions / total_mentions) * 100
            else:
                negative_pct = 0

            row_data.append(negative_pct)

        heatmap_data.append(row_data)

    return np.array(heatmap_data), location_names

def calculate_mention_frequency(df):
    """
    Calculate how often each category is mentioned (regardless of sentiment)
    """
    locations = df['PLACE ADDRESS'].unique()

    frequency_data = []
    location_names = []

    for location in locations:
        location_df = df[df['PLACE ADDRESS'] == location]
        row_data = []

        city = location.split(',')[1].strip() if ',' in location else location[:30]
        location_names.append(city)

        total_reviews = len(location_df)

        for category in CATEGORIES:
            sentiment_col = f'{category}_sentiment'

            # Count non-neutral mentions
            mentions = (location_df[sentiment_col] != 'neutral').sum()
            mention_pct = (mentions / total_reviews) * 100

            row_data.append(mention_pct)

        frequency_data.append(row_data)

    return np.array(frequency_data), location_names

def calculate_weighted_score(df):
    """
    Calculate weighted negative score (percentage * intensity)
    """
    locations = df['PLACE ADDRESS'].unique()

    weighted_data = []
    location_names = []

    for location in locations:
        location_df = df[df['PLACE ADDRESS'] == location]
        row_data = []

        city = location.split(',')[1].strip() if ',' in location else location[:30]
        location_names.append(city)

        for category in CATEGORIES:
            sentiment_col = f'{category}_sentiment'
            intensity_col = f'{category}_intensity'

            # Get negative reviews with their intensities
            negative_mask = location_df[sentiment_col] == 'negative'
            negative_intensities = location_df[negative_mask][intensity_col]

            # Calculate weighted score
            if len(negative_intensities) > 0:
                avg_intensity = negative_intensities.mean()
                negative_count = len(negative_intensities)
                total_reviews = len(location_df)

                # Weighted score: (% negative) * (avg intensity)
                weighted_score = (negative_count / total_reviews * 100) * (avg_intensity / 5)
            else:
                weighted_score = 0

            row_data.append(weighted_score)

        weighted_data.append(row_data)

    return np.array(weighted_data), location_names

def create_heatmap(data, location_names, title, filename, fmt='.1f'):
    """
    Create and save a heatmap visualization
    """
    fig, ax = plt.subplots(figsize=(14, 8))

    # Create heatmap
    sns.heatmap(data,
                annot=True,
                fmt=fmt,
                cmap='RdYlGn_r',  # Red for high (bad), green for low (good)
                xticklabels=[cat.replace('_', ' ').title() for cat in CATEGORIES],
                yticklabels=location_names,
                cbar_kws={'label': 'Score'},
                linewidths=0.5,
                ax=ax)

    plt.title(title, fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('Category', fontsize=12, fontweight='bold')
    plt.ylabel('Location', fontsize=12, fontweight='bold')
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()

    plt.savefig(filename, dpi=300, bbox_inches='tight')
    
    print(f"Saved: {filename}")
    plt.close()

import plotly.graph_objects as go
import plotly.express as px

def create_interactive_heatmap(data, location_names, title, filename):
    """
    Create an interactive heatmap that can be embedded in a website
    """
    fig = go.Figure(data=go.Heatmap(
        z=data,
        x=[cat.replace('_', ' ').title() for cat in CATEGORIES],
        y=location_names,
        colorscale='RdYlGn_r',
        text=data,
        texttemplate='%{text:.1f}',
        textfont={"size": 10},
        colorbar=dict(title="Score")
    ))
    
    fig.update_layout(
        title=title,
        xaxis_title="Category",
        yaxis_title="Location",
        width=1000,
        height=600,
        font=dict(size=12)
    )
    
    # Save as interactive HTML
    html_filename = filename.replace('.png', '.html')
    fig.write_html(html_filename)
    print(f"Saved interactive: {html_filename}")
    
    return fig


def create_category_breakdown(df, category, output_file):
    """
    Create a detailed breakdown for a specific category across locations
    """
    locations = df['PLACE ADDRESS'].unique()

    location_data = []

    for location in locations:
        location_df = df[df['PLACE ADDRESS'] == location]
        city = location.split(',')[1].strip() if ',' in location else location[:30]

        sentiment_col = f'{category}_sentiment'
        intensity_col = f'{category}_intensity'

        positive = (location_df[sentiment_col] == 'positive').sum()
        negative = (location_df[sentiment_col] == 'negative').sum()
        neutral = (location_df[sentiment_col] == 'neutral').sum()

        location_data.append({
            'Location': city,
            'Positive': positive,
            'Negative': negative,
            'Neutral': neutral,
            'Total Mentions': positive + negative
        })

    breakdown_df = pd.DataFrame(location_data)
    breakdown_df = breakdown_df.sort_values('Negative', ascending=False)

    # Create bar chart
    fig, ax = plt.subplots(figsize=(12, 6))

    x = np.arange(len(breakdown_df))
    width = 0.35

    ax.bar(x - width/2, breakdown_df['Negative'], width, label='Negative', color='#e74c3c')
    ax.bar(x + width/2, breakdown_df['Positive'], width, label='Positive', color='#2ecc71')

    ax.set_xlabel('Location', fontweight='bold')
    ax.set_ylabel('Number of Mentions', fontweight='bold')
    ax.set_title(f'{category.replace("_", " ").title()} - Sentiment by Location',
                 fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(breakdown_df['Location'], rotation=45, ha='right')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"Saved: {output_file}")
    plt.close()

    return breakdown_df

def create_priority_matrix(df, output_file):
    """
    Create a priority matrix: Frequency vs Negative Sentiment
    High frequency + high negative = high priority to fix
    """
    category_stats = []

    for category in CATEGORIES:
        sentiment_col = f'{category}_sentiment'

        total_reviews = len(df)
        total_mentions = (df[sentiment_col] != 'neutral').sum()
        negative_mentions = (df[sentiment_col] == 'negative').sum()

        frequency_pct = (total_mentions / total_reviews) * 100

        if total_mentions > 0:
            negative_pct = (negative_mentions / total_mentions) * 100
        else:
            negative_pct = 0

        category_stats.append({
            'Category': category.replace('_', ' ').title(),
            'Frequency': frequency_pct,
            'Negative_Pct': negative_pct,
            'Total_Mentions': total_mentions,
            'Negative_Count': negative_mentions
        })

    stats_df = pd.DataFrame(category_stats)

    # Create scatter plot
    fig, ax = plt.subplots(figsize=(12, 8))

    scatter = ax.scatter(stats_df['Frequency'],
                        stats_df['Negative_Pct'],
                        s=stats_df['Negative_Count'] * 20,  # Size by count
                        alpha=0.6,
                        c=stats_df['Negative_Pct'],
                        cmap='RdYlGn_r')

    # Add labels for each point
    for idx, row in stats_df.iterrows():
        ax.annotate(row['Category'],
                   (row['Frequency'], row['Negative_Pct']),
                   fontsize=9,
                   fontweight='bold',
                   xytext=(5, 5),
                   textcoords='offset points')

    # Add quadrant lines
    ax.axhline(y=stats_df['Negative_Pct'].median(), color='gray', linestyle='--', alpha=0.3)
    ax.axvline(x=stats_df['Frequency'].median(), color='gray', linestyle='--', alpha=0.3)

    ax.set_xlabel('Frequency of Mentions (%)', fontsize=12, fontweight='bold')
    ax.set_ylabel('% Negative When Mentioned', fontsize=12, fontweight='bold')
    ax.set_title('Priority Matrix: Issue Frequency vs Negative Sentiment\n(Bubble size = # of negative mentions)',
                 fontsize=14, fontweight='bold', pad=20)
    ax.grid(True, alpha=0.3)

    # Add quadrant labels
    ax.text(0.95, 0.95, 'HIGH PRIORITY', transform=ax.transAxes,
            fontsize=10, fontweight='bold', color='red',
            ha='right', va='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    plt.colorbar(scatter, label='% Negative', ax=ax)
    plt.tight_layout()
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"Saved: {output_file}")
    plt.close()

    return stats_df

def generate_insights_report(df, output_file):
    """
    Generate a text report with key insights
    """
    with open(output_file, 'w') as f:
        f.write("=" * 80 + "\n")
        f.write("BANDON FITNESS - REVIEW ANALYSIS INSIGHTS REPORT\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 80 + "\n\n")

        # Overall statistics
        f.write("OVERALL STATISTICS\n")
        f.write("-" * 80 + "\n")
        f.write(f"Total Reviews Analyzed: {len(df)}\n")
        f.write(f"Number of Locations: {df['PLACE ADDRESS'].nunique()}\n")
        f.write(f"Average Rating: {df['SCORE'].mean():.2f}/5\n\n")

        # Top issues across all locations
        f.write("TOP ISSUES ACROSS ALL LOCATIONS\n")
        f.write("-" * 80 + "\n")

        issue_counts = []
        for category in CATEGORIES:
            sentiment_col = f'{category}_sentiment'
            negative_count = (df[sentiment_col] == 'negative').sum()
            issue_counts.append((category.replace('_', ' ').title(), negative_count))

        issue_counts.sort(key=lambda x: x[1], reverse=True)

        for i, (issue, count) in enumerate(issue_counts, 1):
            f.write(f"{i}. {issue}: {count} negative mentions\n")

        f.write("\n")

        # Worst performing locations
        f.write("LOCATIONS NEEDING MOST ATTENTION\n")
        f.write("-" * 80 + "\n")

        location_scores = []
        for location in df['PLACE ADDRESS'].unique():
            location_df = df[df['PLACE ADDRESS'] == location]
            city = location.split(',')[1].strip() if ',' in location else location[:30]

            total_negative = 0
            for category in CATEGORIES:
                sentiment_col = f'{category}_sentiment'
                total_negative += (location_df[sentiment_col] == 'negative').sum()

            avg_rating = location_df['SCORE'].mean()
            location_scores.append((city, total_negative, avg_rating, len(location_df)))

        location_scores.sort(key=lambda x: x[1], reverse=True)

        for i, (city, neg_count, avg_rating, review_count) in enumerate(location_scores, 1):
            f.write(f"{i}. {city}\n")
            f.write(f"   - Negative mentions: {neg_count}\n")
            f.write(f"   - Average rating: {avg_rating:.2f}/5\n")
            f.write(f"   - Total reviews: {review_count}\n\n")

        # Category-specific insights
        f.write("CATEGORY-SPECIFIC INSIGHTS\n")
        f.write("-" * 80 + "\n")

        for category in CATEGORIES:
            sentiment_col = f'{category}_sentiment'
            fragments_col = f'{category}_fragments'

            negative_df = df[df[sentiment_col] == 'negative']

            if len(negative_df) > 0:
                f.write(f"\n{category.replace('_', ' ').upper()}\n")
                f.write(f"Negative mentions: {len(negative_df)}\n")

                # Get most common fragments (simple frequency)
                all_fragments = []
                for fragments in negative_df[fragments_col]:
                    if pd.notna(fragments) and fragments != '':
                        all_fragments.extend([f.strip() for f in str(fragments).split('|')])

                if all_fragments:
                    f.write(f"Sample negative comments:\n")
                    for fragment in all_fragments[:3]:
                        f.write(f"  - {fragment}\n")

        f.write("\n" + "=" * 80 + "\n")

    print(f"Saved: {output_file}")

def main(analyzed_file):
    """
    Main function to generate all visualizations
    """
    print("Loading analyzed data...")
    df = load_analyzed_data(analyzed_file)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    print("\n1. Creating main heatmap (% Negative by Location)...")
    negative_data, location_names = calculate_negative_percentage(df)
    create_heatmap(negative_data, location_names,
                  'Bandon Fitness - % Negative Sentiment by Location & Category',
                  f'heatmap_negative_pct_{timestamp}.png')

    print("\n2. Creating mention frequency heatmap...")
    frequency_data, location_names = calculate_mention_frequency(df)
    create_heatmap(frequency_data, location_names,
                  'Bandon Fitness - % Reviews Mentioning Each Category',
                  f'heatmap_mention_frequency_{timestamp}.png')

    print("\n3. Creating weighted severity heatmap...")
    weighted_data, location_names = calculate_weighted_score(df)
    create_heatmap(weighted_data, location_names,
                  'Bandon Fitness - Weighted Negative Score (% Ã— Intensity)',
                  f'heatmap_weighted_score_{timestamp}.png',
                  fmt='.0f')

    print("\n4. Creating priority matrix...")
    priority_stats = create_priority_matrix(df, f'priority_matrix_{timestamp}.png')
    print("\nPriority Rankings:")
    print(priority_stats.sort_values('Negative_Count', ascending=False))

    print("\n5. Creating category breakdowns...")
    top_issues = ['equipment_quality', 'customer_service', 'membership_billing']
    for category in top_issues:
        breakdown = create_category_breakdown(df, category,
                                             f'breakdown_{category}_{timestamp}.png')

    print("\n6. Generating insights report...")
    generate_insights_report(df, f'insights_report_{timestamp}.txt')

    print("\n" + "="*60)
    print("ALL VISUALIZATIONS COMPLETE!")
    print("="*60)
    print("\nGenerated files:")
    print(f"  - heatmap_negative_pct_{timestamp}.png")
    print(f"  - heatmap_mention_frequency_{timestamp}.png")
    print(f"  - heatmap_weighted_score_{timestamp}.png")
    print(f"  - priority_matrix_{timestamp}.png")
    print(f"  - breakdown_*_{timestamp}.png")
    print(f"  - insights_report_{timestamp}.txt")

if __name__ == "__main__":
    # IMPORTANT: Replace with your analyzed Excel file name
    analyzed_file = "temp_Analyzed_Reviews_20251008_151005.xlsx"  # UPDATE THIS!
    main(analyzed_file)